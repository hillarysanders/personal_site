<!DOCTYPE html>
<html lang="en">
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1.0, user-scalable=no"/>
    <title>Hillary Sanders</title>

    <!-- CSS  -->
    <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">
    <link href="css/materialize.css" type="text/css" rel="stylesheet" media="screen,projection"/>
    <link href="css/style.css" type="text/css" rel="stylesheet" media="screen,projection"/>
</head>
<body>

<nav class="blue-grey" role="navigation">
    <div class="nav-wrapper container">

        <!-- Dropdown Structure -->
        <ul id="dd_art" class="dropdown-content blue-grey">
            <li><a href="draw.html" class="white-text">draw</a></li>
            <li><a href="sharpie.html" class="white-text">pen</a></li>
            <li><a href="paint.html" class="white-text">paint</a></li>
            <li><a href="mixed.html" class="white-text">mixed</a></li>
        </ul>
        <ul id="dd_cheatsheets" class="dropdown-content blue-grey">
            <li><a href="python.html" class="white-text">Python</a></li>
            <li><a href="r.html" class="white-text">R</a></li>
            <li><a href="unix.html" class="white-text">UNIX</a></li>
            <li><a href="latex.html" class="white-text">LaTeX</a></li>
            <li><a href="probability.html" class="white-text">Probability</a></li>
        </ul>
        <nav>
            <div class="nav-wrapper">
                <a href="index.html" class="brand-logo hillz">&nbsp;&nbsp;&nbsp;hillz</a>
                <ul class="right hide-on-med-and-down">
                    <li><a href="resume.html">Resume</a></li>
                    <li><a href="recent_projects.html">Recent Projects</a></li>
                    <li><a href="bayesian_statistics.html">Bayesian Statistics</a></li>
                    <li><a href="about_me.html">About Me</a></li>
                    <!-- Dropdown Trigger -->
                    <li><a class="dropdown-button" href="#" data-hover="true" data-beloworigin="true" data-activates="dd_cheatsheets">
                        Cheatsheets
                        <i class="material-icons right">arrow_drop_down</i></a></li>
                    <li><a class="dropdown-button" href="#" data-hover="true" data-beloworigin="true" data-activates="dd_art">
                        Art
                        <i class="material-icons right">arrow_drop_down</i></a></li>
                </ul>
            </div>
        </nav>

        <ul id="nav-mobile" class="side-nav blue-grey white-text">
            <li><a href="index.html" class="white-text">HOME</a></li>
            <li><a href="resume.html" class="white-text">Resume</a></li>
            <li><a href="recent_projects.html" class="white-text">Recent Projects</a></li>
            <li><a href="bayesian_statistics.html" class="white-text">Bayesian Statistics</a></li>
            <li><a href="about_me.html" class="white-text">About Me</a></li>
            <li><a class="white-text">Art</a></li>
            <li><a href="draw.html" class="white-text">&nbsp;&nbsp;&nbsp;draw</a></li>
            <li><a href="sharpie.html" class="white-text">&nbsp;&nbsp;&nbsp;pen</a></li>
            <li><a href="paint.html" class="white-text">&nbsp;&nbsp;&nbsp;paint</a></li>
            <li><a href="mixed.html" class="white-text">&nbsp;&nbsp;&nbsp;mixed</a></li>
            <li><a class="white-text">Cheatsheets</a></li>
            <li><a href="python.html" class="white-text">&nbsp;&nbsp;&nbsp;Python</a></li>
            <li><a href="r.html" class="white-text">&nbsp;&nbsp;&nbsp;R</a></li>
            <li><a href="unix.html" class="white-text">&nbsp;&nbsp;&nbsp;UNIX</a></li>
            <li><a href="latex.html" class="white-text">&nbsp;&nbsp;&nbsp;LaTeX</a></li>
            <li><a href="probability.html" class="white-text">&nbsp;&nbsp;&nbsp;Probability</a></li>
        </ul>
        <a href="#" data-activates="nav-mobile" class="button-collapse"><i class="material-icons">menu</i></a>
    </div>
</nav>

<div class="row center">
    <h2 class="header col s12 white-text">Projects</h2>
</div>

<div class="container">
    <div class="row">
        <div class="col s12">
            <div class="col s12 m6">
                <div class="col s12 recent_projects_box">
                    <h4 class="white-text">For the Fun of It</h4>
                    <div class="cheatbox">
                        <a href="https://noisedive.org/"><h5>Noisedive.org</h5></a>
                        I recently spun up a small website where I could store notes about deep learning whitepapers. Mostly, I wanted a nice place
                        where I could search for, store, and write LaTeX-friendly notes while maintaining an easy-to-use (e.g. Markdown) editing interface.
                    </div>

                    <div class="cheatbox">
                        <a href="https://www.amazon.com/Malware-Data-Science-Detection-Attribution-ebook/dp/B077X1V9SY"><h5>Malware Data Science</h5></a>
                        I teamed up with Joshua Saxe to write "Malware Data Science"
                        (published by <a href="https://nostarch.com/malwaredatascience">No Starch Press</a>),
                        a book that introduces data science techniques for malware
                        detection and analysis. I wrote the two chapters that focus on neural networks, and edited other chapters.
                        <br><br>
                        100% of author proceeds are donated to the Environmental Defense Fund.
                        <br><br>
                        <center><img width="50%" class="materialboxed responsive-img square lazy center" src="images/malware-data-science.jpg"></center>
                    </div>

                    <div class="cheatbox">
                        <a href="http://www.recipestasher.com/"><h5>RescipeStasher</h5></a>
                        A free, ad-less site that can parse free-form recipe text with NLP, enabling unit and
                        servings conversion that actually works and can stack... Unlike all the other ten thousand recipe websites out there :-).
                        It also incorporates (my) art by matching drawings to each recipe's ingredients.
                        <br><br>
                        The site uses a <a href="https://www.djangoproject.com/">django</a> (python) framework
                        (combined with <a href="http://materializecss.com/">materialize</a>, a nice CSS framework) and
                        a <a href="https://www.postgresql.org/">postgreSQL</a> database (hosted on <a href="https://aws.amazon.com/">AWS</a>).
                        The website is deployed via <a href="https://aws.amazon.com/elasticbeanstalk/">AWS Elastic Beanstalk</a>.
                        I wrote all of the code. Yay. :)
                        <br><br>

                        - <a href="http://www.recipestasher.com/">the website</a>
                        <br>
                        - <a href="http://www.recipestasher.com/recipes/detail/lasagna,103/">an example recipe</a>
                        <br>
                        - <a href="http://www.recipestasher.com/cookbook/?search=&public_search=on">some public recipes</a>
                        <br>
                        - <a href="https://github.com/hillarysanders/recipereader">the code (github)</a>
                        <br>
                        - <a href="https://waffle.io/hillarysanders/recipereader">issues waffle board</a>

                    </div>
                    <div class="cheatbox">
                        <a href="bayesian_statistics.html"><h5>Notes on Bayesian Statistics - a booklet</h5></a>
                        The good: it does actually have about 40 pages. And it's all written in lovely <a href="https://www.latex-project.org/">LaTeX</a>.
                        The bad: It's yet unfinished, and has been stagnant for the past three years.
                    </div>
                    <div class="cheatbox">
                        <a href="http://hillarysanders.com/index.html"><h5>This</h5></a>
                        Nah, not a lovely long treatise about Javascript's 'this' - just <i>this</i> website.
                        <br><br>
                        - <a href="https://github.com/hillarysanders/personal_site">the code (github)</a>
                    </div>

                </div>
                <div class="col s12">
                    <spacer>
                        <center>***</center>
                    </spacer>
                </div>
                <div class="col s12 recent_projects_box">
                    <h4 class="white-text">Edu-related Projects (old)</h4>
                    <div class="cheatbox">
                        <h4>edu-related projects</h4>
                        <b></b>
                        <h5> Creating an R Package: Legistative Text Mapping and Directed Acyclic Graphs </h5>
                        From 2011 to 2012 I worked with the fantastic Mark Huberty, a graduate student in Political Science at the Travers Department at
                        UC Berkeley (who is awesome and basically is the reason I got interested in doing what I'm now happily doing with my life. Props.)
                        I helped develop an original R package to map the evolution of legislation from its introduction as a bill, through the amendment
                        process, until finalization. There are five core pieces of functionality in the Leghist package:
                        1. Mapping of amendments to their locations in legislative bills. </br>
                        2. Identification of discarded material. </br>
                        3. Mapping of sections between editions of bills.</br>
                        4. Modeling of the content of added and discarded material.</br>
                        5. Visualization of the flow of bill content, by subject matter.

                        Although I was somewhat involved in all parts of the above, I wrote the code for 5). There were two master functions that I created for
                        the package. Both took raw output from 1:4), and created customizable, yet automated, directed ayclic graphs, implemented through the R
                        package igraph. I also wrote automated scripts to test these functions' functionality.


                        I also worked on figuring out how to document (by implementing Roxygen2), test, and finally create the R package in an automated
                        fashion.


                        <center>
                            <bold> ***</bold>
                        </center>

                        <h5> Automated Pulling, XML Parsing, and Visualization of World Bank Country-wise Economic Indicators.</h5>
                        This was actually for a school project. I worked with a good group of kids, and we all took our own chunks of the intended project and
                        just sort of ran with it. My goal was to completely automate, and make easily adjustable, the automated pulling of World Bank data, and
                        do the same for some really awesome looking, and informative (!) graphs. I would show you the pretty pictures, but I left my laptop out
                        in the rain, so...


                        <center>
                            <bold> ***</bold>
                        </center>

                        <a href="images/midterm.pdf"><h5>Using Tweets and Bayesian Statistical Analysis to Model the 2012 Presidential Election</h5></a>

                        Basically, I created a forecasting model which predicts state-level vote-share probabilities
                        by using a hierarchical Bayesian model to incorporate the simple text analysis of state-specific tweets into predictions. The model used
                        Markov chain Monte Carlo
                        methods to develop the final posterior distribution. Model priors were based off of state-level 2004 and 2008 vote-share data. Data
                        consisted of recent tweets
                        mentioning 'Obama' or 'Romney'. Although the simple text analysis of tweets is a terrible substitute for polling data (problems will be
                        discussed in the paper),
                        it offered a potential way to bolster political forecasting models. (Note: tweets are hideously biased. In most real-life cases, -1 for
                        using them in forecasting models)


                        <center>
                            <bold> ***</bold>
                        </center>

                        <h5> Homegrown Random Forests </h5>

                        This was part of a fun machine learning project I worked on with two other students from Berkeley. Basically we tried to predict
                        baseball
                        players' success by the players' past stats. We implemented a plethora of different machine learning methods; I wrote a random forest
                        learner from scratch. Why? Because. It was pretty fun. Code can be found
                        <a href="pics/random_forest.R" title="Random Forest Code"> <font color="#001070"> here </font> </a> and <a
                            href="pics/random_forest_script.R"> <font color="#001070">
                        here </font> </a>.


                        <center>
                            <bold> ***</bold>
                        </center>

                        <h5> Voting with your Tweet: Forecasting elections with social media data; Broadcasting live predictions online
                        </h5>
                        This project broadcasted live, out-of-sample congressional elections predictions based on Mark Huberty's SuperLearner-based algorithm
                        which takes tweets as input. I helped Mark (who is awesome and taught me nearly everything) by cleaning up code, writing a bit myself,
                        and gathering congressional candidate data.


                    </div>
                </div>
            </div>

            <div class="col s12 m6">
                <div class="col s12 recent_projects_box">

                    <h4 class="white-text">Opendoor - Core Projects</h4>
                    <div class="cheatbox">
                        [TODO!]
                    </div>
                    <h4 class="white-text">Sophos - Core Projects</h4>
                    <div class="cheatbox">
                        <h5>Model Compression</h5>
                        Making models smaller. <a href="https://arxiv.org/abs/2306.10177">arXiv paper</a>, 
                        <a href="images/projects/model-compression-obd-sd-param-level.pdf">original OBD-SD method slides.</a>
                    </div>
                    <div class="cheatbox">
                        <h5>Avoiding Catastrophic Forgetting in Neural Networks</h5>
                        Minimizing the forgetting effects created when you fine-tune trained models on new data.
                        <a href="https://arxiv.org/abs/2306.10181">arXiv paper</a>

                    </div>
                    <div class="cheatbox">
                        <a href="https://pdfs.semanticscholar.org/b410/0c5604bff73d7115765c55f113b4469baee7.pdf"><h5>Garbage In, Garbage Out</h5></a>
                        I wrote this paper to accompany a talk I gave at Black Hat in 2017. The basic idea was to show how malware detection model
                        accuracy can vary wildy based on the test dataset used, which extremely relevant when you're uncertain of your production "test"
                        dataset. To showcase this, I trained our team's URL model on three different datasets, and tested each of these trained models
                        on time-split validation sets from the same three datasets, yielding nine sets of results. I showed how analysing the results can
                        help tell us how sensitive a given model is to changes in the data distribution, and indicate what datasets seem to be supersets of others.

                        The paper for this talk is <a href="https://pdfs.semanticscholar.org/b410/0c5604bff73d7115765c55f113b4469baee7.pdf">here</a>,
                        and the slide deck is <a href="https://www.blackhat.com/docs/us-17/wednesday/us-17-Sanders-Garbage-In-Garbage-Out-How-Purportedly-Great-ML-Models-Can-Be-Screwed-Up-By-Bad-Data.pdf">here</a>.

                    </div>
                    <div class="cheatbox">
                        <h5>Vendor & Model Evaluation Dashboard</h5>
                        Our team develops many models to classify and detect malware. We want to be able to track how our competitors, our deployed models,
                        and our test models are performing against different datasets and label definitions over time. We also want to be able to understand
                        how our own current products will improve (or not) when combined with new detection models.

                        <br><br>

                        To do all of this, another engineer and I created an internal dashboard that runs evalation queries against billions of records in
                        real-time against a redshift database. Users select options like what models and vendors to evaluate, what label definitions to use,
                        and the time period to inspect, and the dashboard runs the resulting SQL queries to find the results, and then generates various
                        interactive
                        plots to showcase the results to the user.

                        <br><br>

                        The dashboard also provides an interface to upload files to be evaluated by our team's models.
                    </div>
                    <div class="cheatbox">
                        <h5>Endpoint Detection And Response</h5>
                        Signatures and Malware detection models aren't always 100% sure if a file is benign, or actually something malicious. When this is
                        the case, or even when a customer's analyst wants to be extra sure about a network event or suspicious file, we want to provide easy
                        ways for an analyst to delve deeper into the threat.

                        <br><br>

                        On this project, I worked with one other engineer to develop docker containers that serve up various models, run queries against an
                        elasticsearch cluster I designed, and output a large JSON object that is served to a customer's dashboard to help them analyze a
                        given potential threat. We developed a single codebase to work for PE, RTF, DOC, and PDF files.
                    </div>
                    <div class="cheatbox">
                        <a href="https://ieeexplore.ieee.org/abstract/document/8424626"><h5>HTML Malware Detection Model</h5></a>
                        I worked on the team to develop our HTML malware detection model. This deep learning model
                        examines the contents of an HTML file at hierarchical spatial scales. Specifically, simple tokens are extracted from an input
                        HTML file from a simple regular expression search, and bin-counts of the numeric hashes of these tokens are analyzed at different
                        scales (like zooming in and out on the file).
                        <br><br>
                        <a href="https://ieeexplore.ieee.org/abstract/document/8424626">The paper for this model is here</a>.

                    </div>
                    <div class="cheatbox">
                        <h5>URL Malware Detection Model</h5>
                        I worked on the team to develop our URL malware detection model. The model is a convolutional neural network, which works by
                        first applying a learned embedding onto characters, and then running 1-d convolutions over these character embeddings. In this way,
                        the neural network is able to develop its own fuzzy-ngram patters, and ends up being able to detect suspicious short-strings with
                        surprising accuracy.
                        <br><br>
                        <a href="https://arxiv.org/abs/1702.08568">The paper for the (older, original) model is here</a>.
                    </div>
<!--                    <div class="cheatbox">-->
<!--                        <h5>Malware Label Analysis</h5>-->

<!--                    </div>-->
                </div>
                <div class="col s12">
                    <spacer>
                        <center>***</center>
                    </spacer>
                </div>
                <div class="col s12 recent_projects_box">
                    <h4 class="white-text">Premise - Core Projects</h4>
                    <div class="cheatbox">
                        <h5>SLA Schema + SLA Ingest and Evaluation Pipelines</h5>
                        The way our company made money was to sign contracts with SLAs (Service Language Agreements) basically saying that we would "gather X
                        data, with Y requirements, and Z restrictions",
                        and then fulfill those contracts by paying contributors money to capture that data. An example contract might require that we capture 30
                        price observations per month (with price SD < .2), and at least five per week
                        for two brands each for twenty specific consumer products, and all of this for each of seven different regions, with at least 9000
                        observations per month.
                        <br><br>
                        This information, while human readable, was not machine readable. So things like progress dashboards had to be written specifically for
                        each new contract, instead of spun up automatically.
                        This wasn't scalable, so I designed a 'SLA' schema that could describe a broad range of SLAs requirements via a JSON blob.
                        <br><br>
                        After I translated all current contracts to this machine-readable format and augmented an existing system to push this data (in
                        google-docs, editable by certain employees) to a redshift db.
                        Then, I wrote a pipeline to track SLAs against their contract's data (ran daily). The pipeline tracked % SLA coverage, as well as
                        budgeting overages, across all facets listed in the SLA
                        (e.g. product X brand X month X region, product X brand X week, product X month, product, etc...)
                    </div>
                    <div class="cheatbox">
                        <h5>Interactive Data-Drill-Down App (Shiny)</h5>
                        Created an interactive web application that allows users to visualize an aggregated time series'
                        component series, or metrics regarding their component series, and drill down to discover more about
                        a specific series. Consisted of about a dozen unique plot types, various search mechanisms,
                        metrics, and options.
                        <br><br>
                        The applications was built entirely in R's <a href="http://shiny.rstudio.com/">Shiny</a> framework.
                    </div>
                    <div class="cheatbox">
                        <h5>Places Clustering</h5>
                        Given many observations, each with:
                        <br><br>
                        <li>longitude, latitude coordinates with non-gaussian error distributions</li>
                        <li>coordinate 'accuracy' (expected SD in meters) metric</li>
                        <li>as associated user (observations within users were highly autocorrelated)</li>
                        <li>a hand-typed (read: messy) place name,</li>
                        <br>
                        I developed a method of clustering these observations to back out 'true' places, and confidence metrics regarding each clustered place's
                        location, name, and actual existence.
                        This was used to help better direct our users when capturing further observations (place based tasks), and used to create better price
                        indices.
                    </div>
                    <!--<div class="cheatbox">-->
                    <!--<h5>Places Coverage Estimation</h5>-->
                    <!--</div>-->

                    <div class="cheatbox">
                        <h5>Optimal Volume Allocation in Surveys</h5>
                        Developed formula (lagrange minimization) to define optimal volume allocation, given a budget, and a goal to minimize
                        the standard error in the end product: an aggregated (over weights) price time series.
                    </div>
                    <div class="cheatbox">
                        <h5>Product Hierarchies & Weights Upkeep System</h5>
                        Developed and maintained system to handle many (quite complex) weights hierarchies. Enforced a series of
                        tests to ensure that end node weights always summed to 1, etc...
                    </div>
                    <div class="cheatbox">
                        <h5>Changepoint, Bimodality detection in price time series</h5>
                        Just another pipeline :). What it sounds like. Data pull, estimation, plots, results push.
                    </div>
                    <div class="cheatbox">
                        <h5>Automated Visualizations of Medicare Data</h5>
                        This is part of what I worked on for the my summer (2012) internship with Acumen LLC. I wrote
                        various R functions which took as input excel workbooks, which the functions parsed, organized, and plotted in some way. Ex:
                        <br>
                        <i>
                            - <code>Fn:</code> Plots normalized values of multiple variables over all districts or all states in the
                            United States with a segmented scatter plot. A state <code>s</code> is highlighted and its values
                            shown. All points above a <code>n</code> standard deviations from the mean become two-letter
                            state abbreviations, or three-letter district abbreviations.
                            <br>
                            - <code>Fn:</code> Allows viewers to spot the professional relationships among (often many thousands of)
                            doctors. First, creates a base distance metric to represent professional closeness among two
                            doctors Di and Dj. This involved variables like the number of beneficiaries Di and Dj share,
                            weighted by billing, the % of beneficiaries Di and Dj share with regards to their own unique
                            beneficiary service count, weighted by billing, etc.
                        </i>
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>


<!--  Scripts-->
<script src="js/jquery-2.2.3.min.js"></script>
<script src="js/materialize.js"></script>
<script src="js/init.js"></script>

</body>
</html>